# ğŸ’¨ Projeto Final - Pipeline com Apache Airflow

Este projeto simula um pipeline de dados utilizando o **Apache Airflow**, que realiza o monitoramento e processamento de arquivos JSON gerados por turbinas eÃ³licas. Os dados sÃ£o armazenados em um banco PostgreSQL e e-mails sÃ£o enviados com base em regras de temperatura.

## ğŸ“Œ Objetivo

- Monitorar arquivos com dados de sensores (JSON).
- Processar os dados com operadores do Airflow.
- Armazenar os dados em um banco PostgreSQL.
- Enviar e-mails de alerta com base na temperatura.

## âš™ï¸ Componentes da DAG

### ğŸ“ **FileSensorTask**
- Verifica se o arquivo existe antes de iniciar a DAG.
- NÃ£o dispara a DAG automaticamente quando o arquivo Ã© criado.
- Usa conexÃ£o `fs_default` (precisa ser criada no Airflow).

### ğŸŒ€ **windturbine (simulador)**
- Gera um arquivo JSON com a seguinte estrutura:
```json
{
  "idtemp": "1",
  "powerfactor": "0.88",
  "hydraulicpressure": "78.86",
  "temperature": "25.27",
  "timestamp": "2023-03-19 17:26:55.230351"
}

Simulado por um notebook Python.

ğŸ PythonOperator
LÃª o conteÃºdo do JSON.

Envia as 5 variÃ¡veis via XCom.

Exclui o arquivo apÃ³s leitura.

ğŸŒ¡ï¸ BranchPythonOperator
Verifica a variÃ¡vel temperature:

Se â‰¥ 24Â°C â†’ envia e-mail de alerta.

Se < 24Â°C â†’ envia e-mail informativo.

ğŸ—ƒï¸ PostgresOperator
Cria a tabela no PostgreSQL (caso nÃ£o exista).

Insere os dados recebidos do JSON.

ğŸ§­ Agendamento
Executa a cada 3 minutos:


![image](https://github.com/user-attachments/assets/8c317dba-f6a1-4412-96ac-03eb6fbea7a0)
